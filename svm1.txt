import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
import pandas as pd

house=pd.read_csv("D:/valli/admission.csv")
X = house.iloc[:, 0:2] # we only take the first two features. We could
 # avoid this ugly slicing by using a two-dim dataset
print(X)
y=house.De

C = 1.0 # SVM regularization parameter
svc = svm.SVC(kernel='linear', C=1,gamma=10).fit(X, y)

#print(X[:,0])
# create a mesh to plot in
x_min=X["GPA"].min() - 1
x_max = X["GPA"].max() + 1
y_min, y_max = X["GMAT"].min() - 1, X["GMAT"].max() + 1
h = (x_max / x_min)/100
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
 np.arange(y_min, y_max, h))


plt.subplot(1, 1, 1)
Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])
print(Z)
Z = Z.reshape(xx.shape)
#plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)

plt.scatter(X["GPA"], X["GMAT"], cmap=plt.cm.Paired)
plt.xlabel('GPA')
plt.ylabel('GMAT')
plt.xlim(xx.min(), xx.max())
plt.title('SVC with linear kernel')
plt.show()